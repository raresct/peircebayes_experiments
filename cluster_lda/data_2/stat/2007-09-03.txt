0709.0165	stat.AP	Of mice and men : Sparse statistical modeling in cardiovascular genomics	In high-throughput genomics , large-scale designed experiments are becoming common , and analysis approaches based on highly multivariate regression and anova concepts are key tools . Shrinkage models of one form or another can provide comprehensive approaches to the problems of simultaneous inference that involve implicit multiple comparisons over the many , many parameters representing effects of design factors and covariates . We use such approaches here in a study of cardiovascular genomics . The primary experimental context concerns a carefully designed , and rich , gene expression study focused on gene-environment interactions , with the goals of identifying genes implicated in connection with disease states and known risk factors , and in generating expression signatures as proxies for such risk factors . A coupled exploratory analysis investigates cross-species extrapolation of gene expression signatures -- how these mouse-model signatures translate to humans . The latter involves exploration of sparse latent factor analysis of human observational data and of how it relates to projected risk signatures derived in the animal models . The study also highlights a range of applied statistical and genomic data analysis issues , including model specification , computational questions and model-based correction of experimental artifacts in DNA microarray data .
0709.0258	stat.ME	Networks of Polynomial Pieces with Application to the Analysis of Point Clouds and Images	We consider Holder smoothness classes of surfaces for which we construct piecewise polynomial approximation networks , which are graphs with polynomial pieces as nodes and edges between polynomial pieces that are in `good continuation ' of each other . Little known to the community , a similar construction was used by Kolmogorov and Tikhomirov in their proof of their celebrated entropy results for Holder classes . We show how to use such networks in the context of detecting geometric objects buried in noise to approximate the scan statistic , yielding an optimization problem akin to the Traveling Salesman . In the same context , we describe an alternative approach based on computing the longest path in the network after appropriate thresholding . For the special case of curves , we also formalize the notion of `good continuation ' between beamlets in any dimension , obtaining more economical piecewise linear approximation networks for curves . We include some numerical experiments illustrating the use of the beamlet network in characterizing the filamentarity content of 3D datasets , and show that even a rudimentary notion of good continuity may bring substantial improvement .
