0707.0143	stat.ME	On semiparametric regression with O'Sullivan penalised splines	This is an expos\'e on the use of O'Sullivan penalised splines in contemporary semiparametric regression , including mixed model and Bayesian formulations . O'Sullivan penalised splines are similar to P-splines , but have an advantage of being a direct generalisation of smoothing splines . Exact expressions for the O'Sullivan penalty matrix are obtained . Comparisons between the two reveals that O'Sullivan penalised splines more closely mimic the natural boundary behaviour of smoothing splines . Implementation in modern computing environments such as Matlab , R and BUGS is discussed .
0707.0167	stat.CO	The random Tukey depth	The computation of the Tukey depth , also called halfspace depth , is very demanding , even in low dimensional spaces , because it requires the consideration of all possible one-dimensional projections . In this paper we propose a random depth which approximates the Tukey depth . It only takes into account a finite number of one-dimensional projections which are chosen at random . Thus , this random depth requires a very small computation time even in high dimensional spaces . Moreover , it is easily extended to cover the functional framework . We present some simulations indicating how many projections should be considered depending on the sample size and on the dimension of the sample space . We also compare this depth with some others proposed in the literature . It is noteworthy that the random depth , based on a very low number of projections , obtains results very similar to those obtained with other depths .
0707.0246	stat.ME	A new graphical tool of outliers detection in regression models based on recursive estimation	We present in this paper a new tool for outliers detection in the context of multiple regression models . This graphical tool is based on recursive estimation of the parameters . Simulations were carried out to illustrate the performance of this graphical procedure . As a conclusion , this tool is applied to real data containing outliers according to the classical available tools .
0707.0303	stat.ML stat.ME	Learning from dependent observations	In most papers establishing consistency for learning algorithms it is assumed that the observations used for training are realizations of an i.i.d . process . In this paper we go far beyond this classical framework by showing that support vector machines ( SVMs ) essentially only require that the data-generating process satisfies a certain law of large numbers . We then consider the learnability of SVMs for $ \a $ -mixing ( not necessarily stationary ) processes for both classification and regression , where for the latter we explicitly allow unbounded noise .
0707.0322	stat.ME math.DS math.ST stat.TH	Consistency of support vector machines for forecasting the evolution of an unknown ergodic dynamical system from observations with unknown noise	We consider the problem of forecasting the next ( observable ) state of an unknown ergodic dynamical system from a noisy observation of the present state . Our main result shows , for example , that support vector machines ( SVMs ) using Gaussian RBF kernels can learn the best forecaster from a sequence of noisy observations if ( a ) the unknown observational noise process is bounded and has a summable $ \alpha $ -mixing rate and ( b ) the unknown ergodic dynamical system is defined by a Lipschitz continuous function on some compact subset of $ \mathbb { R } ^d $ and has a summable decay of correlations for Lipschitz continuous functions . In order to prove this result we first establish a general consistency result for SVMs and all stochastic processes that satisfy a mixing notion that is substantially weaker than $ \alpha $ -mixing .
