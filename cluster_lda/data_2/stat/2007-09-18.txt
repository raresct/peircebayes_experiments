0709.2760	stat.ML	Supervised Machine Learning with a Novel Kernel Density Estimator	In recent years , kernel density estimation has been exploited by computer scientists to model machine learning problems . The kernel density estimation based approaches are of interest due to the low time complexity of either O ( n ) or O ( n*log ( n ) ) for constructing a classifier , where n is the number of sampling instances . Concerning design of kernel density estimators , one essential issue is how fast the pointwise mean square error ( MSE ) and/or the integrated mean square error ( IMSE ) diminish as the number of sampling instances increases . In this article , it is shown that with the proposed kernel function it is feasible to make the pointwise MSE of the density estimator converge at O ( n^-2/3 ) regardless of the dimension of the vector space , provided that the probability density function at the point of interest meets certain conditions .
0709.2776	stat.ME math.CO stat.CO	A note on calculating autocovariances of periodic ARMA models	An analytically simple and tractable formula for the start-up autocovariances of periodic ARMA ( PARMA ) models is provided .
0709.2936	stat.ML	Bayesian Classification and Regression with High Dimensional Features	This thesis responds to the challenges of using a large number , such as thousands , of features in regression and classification problems . There are two situations where such high dimensional features arise . One is when high dimensional measurements are available , for example , gene expression data produced by microarray techniques . For computational or other reasons , people may select only a small subset of features when modelling such data , by looking at how relevant the features are to predicting the response , based on some measure such as correlation with the response in the training data . Although it is used very commonly , this procedure will make the response appear more predictable than it actually is . In Chapter 2 , we propose a Bayesian method to avoid this selection bias , with application to naive Bayes models and mixture models . High dimensional features also arise when we consider high-order interactions . The number of parameters will increase exponentially with the order considered . In Chapter 3 , we propose a method for compressing a group of parameters into a single one , by exploiting the fact that many predictor variables derived from high-order interactions have the same values for all the training cases . The number of compressed parameters may have converged before considering the highest possible order . We apply this compression method to logistic sequence prediction models and logistic classification models . We use both simulated data and real data to test our methods in both chapters .
0709.2943	stat.ME math.ST stat.TH	On Birnbaum-Saunders Inference	The Birnbaum-Saunders distribution , also known as the fatigue-life distribution , is frequently used in reliability studies . We obtain adjustments to the Birnbaum -- Saunders profile likelihood function . The modified versions of the likelihood function were obtained for both the shape and scale parameters , i.e. , we take the shape parameter to be of interest and the scale parameter to be of nuisance , and then consider the situation in which the interest lies in performing inference on the scale parameter with the shape parameter entering the modeling in nuisance fashion . Modified profile maximum likelihood estimators are obtained by maximizing the corresponding adjusted likelihood functions . We present numerical evidence on the finite sample behavior of the different estimators and associated likelihood ratio tests . The results favor the adjusted estimators and tests we propose . A novel aspect of the profile likelihood adjustments obtained in this paper is that they yield improved point estimators and tests . The two profile likelihood adjustments work well when inference is made on the shape parameter , and one of them displays superior behavior when it comes to performing hypothesis testing inference on the scale parameter . Two empirical applications are briefly presented .
