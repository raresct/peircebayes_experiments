0708.0279	stat.ME	Expert Elicitation for Reliable System Design	This paper reviews the role of expert judgement to support reliability assessments within the systems engineering design process . Generic design processes are described to give the context and a discussion is given about the nature of the reliability assessments required in the different systems engineering phases . It is argued that , as far as meeting reliability requirements is concerned , the whole design process is more akin to a statistical control process than to a straightforward statistical problem of assessing an unknown distribution . This leads to features of the expert judgement problem in the design context which are substantially different from those seen , for example , in risk assessment . In particular , the role of experts in problem structuring and in developing failure mitigation options is much more prominent , and there is a need to take into account the reliability potential for future mitigation measures downstream in the system life cycle . An overview is given of the stakeholders typically involved in large scale systems engineering design projects , and this is used to argue the need for methods that expose potential judgemental biases in order to generate analyses that can be said to provide rational consensus about uncertainties . Finally , a number of key points are developed with the aim of moving toward a framework that provides a holistic method for tracking reliability assessment through the design process .
0708.0285	stat.ME	Comment : Expert Elicitation for Reliable System Design	Comment : Expert Elicitation for Reliable System Design [ arXiv:0708.0279 ]
0708.0287	stat.ME	Comment : Expert Elicitation for Reliable System Design	Comment : Expert Elicitation for Reliable System Design [ arXiv:0708.0279 ]
0708.0288	stat.ME	Comment : Expert Elicitation for Reliable System Design	Comment : Expert Elicitation for Reliable System Design [ arXiv:0708.0279 ]
0708.0293	stat.ME	Rejoinder : Expert Elicitation for Reliable System Design	Rejoinder : Expert Elicitation for Reliable System Design [ arXiv:0708.0279 ]
0708.0295	stat.ME	Reliability	This special volume of Statistical Sciences presents some innovative , if not provocative , ideas in the area of reliability , or perhaps more appropriately named , integrated system assessment . In this age of exponential growth in science , engineering and technology , the capability to evaluate the performance , reliability and safety of complex systems presents new challenges . Today 's methodology must respond to the ever-increasing demands for such evaluations to provide key information for decision and policy makers at all levels of government and industry -- problems ranging from international security to space exploration . We , the co-editors of this volume and the authors , believe that scientific progress in reliability assessment requires the development of processes , methods and tools that combine diverse information types ( e.g. , experiments , computer simulations , expert knowledge ) from diverse sources ( e.g. , scientists , engineers , business developers , technology integrators , decision makers ) to assess quantitative performance metrics that can aid decision making under uncertainty . These are highly interdisciplinary problems . The principal role of statistical sciences is to bring statistical rigor , thinking and methodology to these problems .
0708.0302	stat.ME	Monitoring Networked Applications With Incremental Quantile Estimation	Networked applications have software components that reside on different computers . Email , for example , has database , processing , and user interface components that can be distributed across a network and shared by users in different locations or work groups . End-to-end performance and reliability metrics describe the software quality experienced by these groups of users , taking into account all the software components in the pipeline . Each user produces only some of the data needed to understand the quality of the application for the group , so group performance metrics are obtained by combining summary statistics that each end computer periodically ( and automatically ) sends to a central server . The group quality metrics usually focus on medians and tail quantiles rather than on averages . Distributed quantile estimation is challenging , though , especially when passing large amounts of data around the network solely to compute quality metrics is undesirable . This paper describes an Incremental Quantile ( IQ ) estimation method that is designed for performance monitoring at arbitrary levels of network aggregation and time resolution when only a limited amount of data can be transferred . Applications to both real and simulated data are provided .
0708.0317	stat.ME	Comment : Monitoring Networked Applications With Incremental Quantile Estimation	Comment : Monitoring Networked Applications With Incremental Quantile Estimation [ arXiv:0708.0302 ]
0708.0336	stat.ME	Comment : Monitoring Networked Applications With Incremental Quantile Estimation	Our comments are in two parts . First , we make some observations regarding the methodology in Chambers et al . [ arXiv:0708.0302 ] . Second , we briefly describe another interesting network monitoring problem that arises in the context of assessing quality of service , such as loss rates and delay distributions , in packet-switched networks .
0708.0338	stat.ME	Comment : Monitoring Networked Applications With Incremental Quantile Estimation	Comment : Monitoring Networked Applications With Incremental Quantile Estimation [ arXiv:0708.0302 ]
0708.0339	stat.ME	Rejoinder : Monitoring Networked Applications With Incremental Quantile Estimation	Rejoinder : Monitoring Networked Applications With Incremental Quantile Estimation [ arXiv:0708.0302 ]
0708.0343	stat.ME	Dynamic Modeling and Statistical Analysis of Event Times	This review article provides an overview of recent work in the modeling and analysis of recurrent events arising in engineering , reliability , public health , biomedicine and other areas . Recurrent event modeling possesses unique facets making it different and more difficult to handle than single event settings . For instance , the impact of an increasing number of event occurrences needs to be taken into account , the effects of covariates should be considered , potential association among the interevent times within a unit can not be ignored , and the effects of performed interventions after each event occurrence need to be factored in . A recent general class of models for recurrent events which simultaneously accommodates these aspects is described . Statistical inference methods for this class of models are presented and illustrated through applications to real data sets . Some existing open research problems are described .
0708.0346	stat.ME	Threshold Regression for Survival Analysis : Modeling Event Times by a Stochastic Process Reaching a Boundary	Many researchers have investigated first hitting times as models for survival data . First hitting times arise naturally in many types of stochastic processes , ranging from Wiener processes to Markov chains . In a survival context , the state of the underlying process represents the strength of an item or the health of an individual . The item fails or the individual experiences a clinical endpoint when the process reaches an adverse threshold state for the first time . The time scale can be calendar time or some other operational measure of degradation or disease progression . In many applications , the process is latent ( i.e. , unobservable ) . Threshold regression refers to first-hitting-time models with regression structures that accommodate covariate data . The parameters of the process , threshold state and time scale may depend on the covariates . This paper reviews aspects of this topic and discusses fruitful avenues for future research .
0708.0355	stat.ME	Advances in Data Combination , Analysis and Collection for System Reliability Assessment	The systems that statisticians are asked to assess , such as nuclear weapons , infrastructure networks , supercomputer codes and munitions , have become increasingly complex . It is often costly to conduct full system tests . As such , we present a review of methodology that has been proposed for addressing system reliability with limited full system testing . The first approaches presented in this paper are concerned with the combination of multiple sources of information to assess the reliability of a single component . The second general set of methodology addresses the combination of multiple levels of data to determine system reliability . We then present developments for complex systems beyond traditional series/parallel representations through the use of Bayesian networks and flowgraph models . We also include methodological contributions to resource allocation considerations for system relability assessment . We illustrate each method with applications primarily encountered at Los Alamos National Laboratory .
0708.0362	stat.ME	On the Statistical Modeling and Analysis of Repairable Systems	We review basic modeling approaches for failure and maintenance data from repairable systems . In particular we consider imperfect repair models , defined in terms of virtual age processes , and the trend-renewal process which extends the nonhomogeneous Poisson process and the renewal process . In the case where several systems of the same kind are observed , we show how observed covariates and unobserved heterogeneity can be included in the models . We also consider various approaches to trend testing . Modern reliability data bases usually contain information on the type of failure , the type of maintenance and so forth in addition to the failure times themselves . Basing our work on recent literature we present a framework where the observed events are modeled as marked point processes , with marks labeling the types of events . Throughout the paper the emphasis is more on modeling than on statistical inference .
0708.0369	stat.ME	A Review of Accelerated Test Models	Engineers in the manufacturing industries have used accelerated test ( AT ) experiments for many decades . The purpose of AT experiments is to acquire reliability information quickly . Test units of a material , component , subsystem or entire systems are subjected to higher-than-usual levels of one or more accelerating variables such as temperature or stress . Then the AT results are used to predict life of the units at use conditions . The extrapolation is typically justified ( correctly or incorrectly ) on the basis of physically motivated models or a combination of empirical model fitting with a sufficient amount of previous experience in testing similar units . The need to extrapolate in both time and the accelerating variables generally necessitates the use of fully parametric models . Statisticians have made important contributions in the development of appropriate stochastic models for AT data [ typically a distribution for the response and regression relationships between the parameters of this distribution and the accelerating variable ( s ) ] , statistical methods for AT planning ( choice of accelerating variable levels and allocation of available test units to those levels ) and methods of estimation of suitable reliability metrics . This paper provides a review of many of the AT models that have been used successfully in this area .
0708.0378	stat.ME	A Conversation With Harry Martz	Harry F. Martz was born June 16 , 1942 and grew up in Cumberland , Maryland . He received a Bachelor of Science degree in mathematics ( with a minor in physics ) from Frostburg State University in 1964 , and earned a Ph.D. in statistics at Virginia Polytechnic Institute and State University in 1968 . He started his statistics career at Texas Tech University 's Department of Industrial Engineering and Statistics right after graduation . In 1978 , he joined the technical staff at Los Alamos National Laboratory ( LANL ) in Los Alamos , New Mexico after first working as Full Professor in the Department of Industrial Engineering at Utah State University in the fall of 1977 . He has had a prolific 23-year career with the statistics group at LANL ; over the course of his career , Martz has published over 80 research papers in books and refereed journals , one book ( with co-author Ray Waller ) , and has four patents associated with his work at LANL . He is a fellow of the American Statistical Association and has received numerous awards , including the Technometrics Frank Wilcoxon Prize for Best Applications Paper ( 1996 ) , Los Alamos National Laboratory Achievement Award ( 1998 ) , R & D 100 Award by R & D Magazine ( 2003 ) , Council for Chemical Research Collaboration Success Award ( 2004 ) , and Los Alamos National Laboratory 's Distinguished Licensing Award ( 2004 ) . Since retiring as a Technical Staff member at LANL in 2001 , he has worked as a LANL Laboratory Associate .
