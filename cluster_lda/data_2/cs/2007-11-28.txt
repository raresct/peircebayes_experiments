0711.4380	cs.IT math.IT	Randomness and metastability in CDMA paradigms	Code Division Multiple Access ( CDMA ) in which the signature code assignment to users contains a random element has recently become a cornerstone of CDMA research . The random element in the construction is particularly attractive in that it provides robustness and flexibility in application , whilst not making significant sacrifices in terms of multiuser efficiency . We present results for sparse random codes of two types , with and without modulation . Simple microscopic consideration on system samples would suggest differences in the phase space of the two models , but we demonstrate that the thermodynamic results and metastable states are equivalent in the minimum bit error rate detector . We analyse marginal properties of interactions and also make analogies to constraint satisfiability problems in order to understand qualitative features of detection and metastable states . This may have consequences for developing algorithmic methods to escape metastable states , thus improving decoding performance .
0711.4444	cs.MS cs.CE	Building the Tangent and Adjoint codes of the Ocean General Circulation Model OPA with the Automatic Differentiation tool TAPENADE	The ocean general circulation model OPA is developed by the LODYC team at Paris VI university . OPA has recently undergone a major rewriting , migrating to FORTRAN95 , and its adjoint code needs to be rebuilt . For earlier versions , the adjoint of OPA was written by hand at a high development cost . We use the Automatic Differentiation tool TAPENADE to build mechanicaly the tangent and adjoint codes of OPA . We validate the differentiated codes by comparison with divided differences , and also with an identical twin experiment . We apply state-of-the-art methods to improve the performance of the adjoint code . In particular we implement the Griewank and Walther 's binomial checkpointing algorithm which gives us an optimal trade-off between time and memory consumption . We apply a specific strategy to differentiate the iterative linear solver that comes from the implicit time stepping scheme
0711.4452	cs.LG	Covariance and PCA for Categorical Variables	Covariances from categorical variables are defined using a regular simplex expression for categories . The method follows the variance definition by Gini , and it gives the covariance as a solution of simultaneous equations . The calculated results give reasonable values for test data . A method of principal component analysis ( RS-PCA ) is also proposed using regular simplex expressions , which allows easy interpretation of the principal components . The proposed methods apply to variable selection problem of categorical data USCensus1990 data . The proposed methods give appropriate criterion for the variable selection problem of categorical
0711.4475	cs.CL	Valence extraction using EM selection and co-occurrence matrices	This paper discusses two new procedures for extracting verb valences from raw texts , with an application to the Polish language . The first novel technique , the EM selection algorithm , performs unsupervised disambiguation of valence frame forests , obtained by applying a non-probabilistic deep grammar parser and some post-processing to the text . The second new idea concerns filtering of incorrect frames detected in the parsed text and is motivated by an observation that verbs which take similar arguments tend to have similar frames . This phenomenon is described in terms of newly introduced co-occurrence matrices . Using co-occurrence matrices , we split filtering into two steps . The list of valid arguments is first determined for each verb , whereas the pattern according to which the arguments are combined into frames is computed in the following stage . Our best extracted dictionary reaches an $ F $ -score of 45 % , compared to an $ F $ -score of 39 % for the standard frame-based BHT filtering .
0711.4507	cs.IT cs.AI math.IT	The Second Law as a Cause of the Evolution	It is a common belief that in any environment where life is possible , life will be generated . Here it is suggested that the cause for a spontaneous generation of complex systems is probability driven processes . Based on equilibrium thermodynamics , it is argued that in low occupation number statistical systems , the second law of thermodynamics yields an increase of thermal entropy and a canonic energy distribution . However , in high occupation number statistical systems , the same law for the same reasons yields an increase of information and a Benford 's law/power-law energy distribution . It is therefore , plausible , that eventually the heat death is not necessarily the end of the universe .
0711.4508	cs.CC cs.CV cs.IT math.IT	Representation and Measure of Structural Information	We introduce a uniform representation of general objects that captures the regularities with respect to their structure . It allows a representation of a general class of objects including geometric patterns and images in a sparse , modular , hierarchical , and recursive manner . The representation can exploit any computable regularity in objects to compactly describe them , while also being capable of representing random objects as raw data . A set of rules uniformly dictates the interpretation of the representation into raw signal , which makes it possible to ask what pattern a given raw signal contains . Also , it allows simple separation of the information that we wish to ignore from that which we measure , by using a set of maps to delineate the a priori parts of the objects , leaving only the information in the structure . Using the representation , we introduce a measure of information in general objects relative to structures defined by the set of maps . We point out that the common prescription of encoding objects by strings to use Kolmogorov complexity is meaningless when , as often is the case , the encoding is not specified in any way other than that it exists . Noting this , we define the measure directly in terms of the structures of the spaces in which the objects reside . As a result , the measure is defined relative to a set of maps that characterize the structures . It turns out that the measure is equivalent to Kolmogorov complexity when it is defined relative to the maps characterizing the structure of natural numbers . Thus , the formulation gives the larger class of objects a meaningful measure of information that generalizes Kolmogorov complexity .
0711.4516	cs.OH	Fluoroscopy-based navigation system in spine surgery	The variability in width , height , and spatial orientation of a spinal pedicle makes pedicle screw insertion a delicate operation . The aim of the current paper is to describe a computer-assisted surgical navigation system based on fluoroscopic X-ray image calibration and three-dimensional optical localizers in order to reduce radiation exposure while increasing accuracy and reliability of the surgical procedure for pedicle screw insertion . Instrumentation using transpedicular screw fixation was performed : in a first group , a conventional surgical procedure was carried out with 26 patients ( 138 screws ) ; in a second group , a navigated surgical procedure ( virtual fluoroscopy ) was performed with 26 patients ( 140 screws ) . Evaluation of screw placement in every case was done by using plain X-rays and post-operative computer tomography scan . A 5 per cent cortex penetration ( 7 of 140 pedicle screws ) occurred for the computer-assisted group . A 13 per cent penetration ( 18 of 138 pedicle screws ) occurred for the non computer-assisted group . The radiation running time for each vertebra level ( two screws ) reached 3.5 s on average in the computer-assisted group and 11.5 s on average in the non computer-assisted group . The operative time for two screws on the same vertebra level reaches 10 min on average in the non computer-assisted group and 11.9 min on average in the computer-assisted group . The fluoroscopy-based ( two-dimensional ) navigation system for pedicle screw insertion is a safe and reliable procedure for surgery in the lower thoracic and lumbar spine .
0711.4523	cs.OH	Robot-based tele-echography : clinical evaluation of the TER system in abdominal aortic exploration	OBJECTIVE : The TER system is a robot-based tele-echography system allowing remote ultrasound examination . The specialist moves a mock-up of the ultrasound probe at the master site , and the robot reproduces the movements of the real probe , which sends back ultrasound images and force feedback . This tool could be used to perform ultrasound examinations in small health care centers or from isolated sites . The objective of this study was to prove , under real conditions , the feasibility and reliability of the TER system in detecting abdominal aortic and iliac aneurysms . METHODS : Fifty-eight patients were included in 2 centers in Brest and Grenoble , France . The remote examination was compared with the reference standard , the bedside examination , for aorta and iliac artery diameter measurement , detection and description of aneurysms , detection of atheromatosis , the duration of the examination , and acceptability . RESULTS : All aneurysms ( 8 ) were detected by both techniques as intramural thrombosis and extension to the iliac arteries . The interobserver correlation coefficient was 0.982 ( P < .0001 ) for aortic diameters . The rate of concordance between 2 operators in evaluating atheromatosis was 84 % +/- 11 % ( 95 % confidence interval ) . CONCLUSIONS : Our study on 58 patients suggests that the TER system could be a reliable , acceptable , and effective robot-based system for performing remote abdominal aortic ultrasound examinations . Research is continuing to improve the equipment for general abdominal use .
0711.4557	cs.IT math.IT	On Outage Behavior of Wideband Slow-Fading Channels	This paper investigates point-to-point information transmission over a wideband slow-fading channel , modeled as an ( asymptotically ) large number of independent identically distributed parallel channels , with the random channel fading realizations remaining constant over the entire coding block . On the one hand , in the wideband limit the minimum achievable energy per nat required for reliable transmission , as a random variable , converges in probability to certain deterministic quantity . On the other hand , the exponential decay rate of the outage probability , termed as the wideband outage exponent , characterizes how the number of parallel channels , { \it i.e . } , the `` bandwidth '' , should asymptotically scale in order to achieve a target outage probability at a target energy per nat . We examine two scenarios : when the transmitter has no channel state information and adopts uniform transmit power allocation among parallel channels ; and when the transmitter is endowed with an one-bit channel state feedback for each parallel channel and accordingly allocates its transmit power . For both scenarios , we evaluate the wideband minimum energy per nat and the wideband outage exponent , and discuss their implication for system performance .
0711.4562	cs.NI	Near-Deterministic Inference of AS Relationships	The discovery of Autonomous Systems ( ASes ) interconnections and the inference of their commercial Type-of-Relationships ( ToR ) has been extensively studied during the last few years . The main motivation is to accurately calculate AS-level paths and to provide better topological view of the Internet . An inherent problem in current algorithms is their extensive use of heuristics . Such heuristics incur unbounded errors which are spread over all inferred relationships . We propose a near-deterministic algorithm for solving the ToR inference problem . Our algorithm uses as input the Internet core , which is a dense sub-graph of top-level ASes . We test several methods for creating such a core and demonstrate the robustness of the algorithm to the core 's size and density , the inference period , and errors in the core . We evaluate our algorithm using AS-level paths collected from RouteViews BGP paths and DIMES traceroute measurements . Our proposed algorithm deterministically infers over 95 % of the approximately 58,000 AS topology links . The inference becomes stable when using a week worth of data and as little as 20 ASes in the core . The algorithm infers 2-3 times more peer-to-peer relationships in edges discovered only by DIMES than in RouteViews edges , validating the DIMES promise to discover periphery AS edges .
0711.4573	cs.DS	A Note On Computing Set Overlap Classes	Let $ { \cal V } $ be a finite set of $ n $ elements and $ { \cal F } =\ { X_1 , X_2 , > ... , X_m\ } $ a family of $ m $ subsets of $ { \cal V } . $ Two sets $ X_i $ and $ X_j $ of $ { \cal F } $ overlap if $ X_i \cap X_j \neq \emptyset , $ $ X_j \setminus X_i \neq \emptyset , $ and $ X_i \setminus X_j \neq \emptyset. $ Two sets $ X , Y\in { \cal F } $ are in the same overlap class if there is a series $ X=X_1 , X_2 , ... , X_k=Y $ of sets of $ { \cal F } $ in which each $ X_iX_ { i+1 } $ overlaps . In this note , we focus on efficiently identifying all overlap classes in $ O ( n+\sum_ { i=1 } ^m |X_i| ) $ time . We thus revisit the clever algorithm of Dahlhaus of which we give a clear presentation and that we simplify to make it practical and implementable in its real worst case complexity . An useful variant of Dahlhaus 's approach is also explained .
0711.4613	cs.CY	Spreadsheet Risk - A New Direction for HMRC ?	Her Majestys Revenue & Customs ( HMRC ) was born out of the need to create a UK tax authority by merging both the Inland Revenue and HM Customs & Excise into one department . HMRC encounters spreadsheets in tax-payers systems on a very regular basis as well as being a heavy user of spreadsheets internally . The approach to spreadsheet risk assessment and spreadsheet audit is by the use of trained computer auditors and data handlers . This , by definition , limits the use of our specialist spreadsheet audit tool to such trained staff . In order to tackle the growing use of spreadsheets , a new way of approaching the problem has been piloted . The aim is to issue all staff who come across spreadsheets with a simple to use analysis and risk assessment tool , based on the departmental software SpACE ( Spreadsheet Audit & Compliance Examination ) .
0711.4634	cs.CY	Strategies for Addressing Spreadsheet Compliance Challenges	Most organizations today use spreadsheets in some form or another to support critical business processes . However the financial resources , and developmental rigor dedicated to them are often minor in comparison to other enterprise technology . The increasing focus on achieving regulatory and other forms of compliance over key technology assets has made it clear that organizations must regard spreadsheets as an enterprise resource and account for them when developing an overall compliance strategy . This paper provides the reader with a set of practical strategies for addressing spreadsheet compliance from an organizational perspective . It then presents capabilities offered in the 2007 Microsoft Office System which can be used to help customers address compliance challenges .
