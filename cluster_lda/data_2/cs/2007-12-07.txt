0712.1097	cs.AI cs.DS	On Using Unsatisfiability for Solving Maximum Satisfiability	Maximum Satisfiability ( MaxSAT ) is a well-known optimization pro- blem , with several practical applications . The most widely known MAXS AT algorithms are ineffective at solving hard problems instances from practical application domains . Recent work proposed using efficient Boolean Satisfiability ( SAT ) solvers for solving the MaxSAT problem , based on identifying and eliminating unsatisfiable subformulas . However , these algorithms do not scale in practice . This paper analyzes existing MaxSAT algorithms based on unsatisfiable subformula identification . Moreover , the paper proposes a number of key optimizations to these MaxSAT algorithms and a new alternative algorithm . The proposed optimizations and the new algorithm provide significant performance improvements on MaxSAT instances from practical applications . Moreover , the efficiency of the new generation of unsatisfiability-based MaxSAT solvers becomes effectively indexed to the ability of modern SAT solvers to proving unsatisfiability and identifying unsatisfiable subformulas .
0712.1163	cs.DS cond-mat.dis-nn cs.DM physics.soc-ph	Efficient modularity optimization by multistep greedy algorithm and vertex mover refinement	Identifying strongly connected substructures in large networks provides insight into their coarse-grained organization . Several approaches based on the optimization of a quality function , e.g. , the modularity , have been proposed . We present here a multistep extension of the greedy algorithm ( MSG ) that allows the merging of more than one pair of communities at each iteration step . The essential idea is to prevent the premature condensation into few large communities . Upon convergence of the MSG a simple refinement procedure called '' vertex mover '' ( VM ) is used for reassigning vertices to neighboring communities to improve the final modularity value . With an appropriate choice of the step width , the combined MSG-VM algorithm is able to find solutions of higher modularity than those reported previously . The multistep extension does not alter the scaling of computational cost of the greedy algorithm .
0712.1167	cs.AR cs.DC	Transactional WaveCache : Towards Speculative and Out-of-Order DataFlow Execution of Memory Operations	The WaveScalar is the first DataFlow Architecture that can efficiently provide the sequential memory semantics required by imperative languages . This work presents an alternative memory ordering mechanism for this architecture , the Transaction WaveCache . Our mechanism maintains the execution order of memory operations within blocks of code , called Waves , but adds the ability to speculatively execute , out-of-order , operations from different waves . This ordering mechanism is inspired by progress in supporting Transactional Memories . Waves are considered as atomic regions and executed as nested transactions . If a wave has finished the execution of all its memory operations , as soon as the previous waves are committed , it can be committed . If a hazard is detected in a speculative Wave , all the following Waves ( children ) are aborted and re-executed . We evaluate the WaveCache on a set artificial benchmarks . If the benchmark does not access memory often , we could achieve speedups of around 90 % . Speedups of 33.1 % and 24 % were observed on more memory intensive applications , and slowdowns up to 16 % arise if memory bandwidth is a bottleneck . For an application full of WAW , WAR and RAW hazards , a speedup of 139.7 % was verified .
0712.1169	cs.IT math.IT	Opportunistic Relaying in Wireless Networks	Relay networks having $ n $ source-to-destination pairs and $ m $ half-duplex relays , all operating in the same frequency band in the presence of block fading , are analyzed . This setup has attracted significant attention and several relaying protocols have been reported in the literature . However , most of the proposed solutions require either centrally coordinated scheduling or detailed channel state information ( CSI ) at the transmitter side . Here , an opportunistic relaying scheme is proposed , which alleviates these limitations . The scheme entails a two-hop communication protocol , in which sources communicate with destinations only through half-duplex relays . The key idea is to schedule at each hop only a subset of nodes that can benefit from \emph { multiuser diversity } . To select the source and destination nodes for each hop , it requires only CSI at receivers ( relays for the first hop , and destination nodes for the second hop ) and an integer-value CSI feedback to the transmitters . For the case when $ n $ is large and $ m $ is fixed , it is shown that the proposed scheme achieves a system throughput of $ m/2 $ bits/s/Hz . In contrast , the information-theoretic upper bound of $ ( m/2 ) \log \log n $ bits/s/Hz is achievable only with more demanding CSI assumptions and cooperation between the relays . Furthermore , it is shown that , under the condition that the product of block duration and system bandwidth scales faster than $ \log n $ , the achievable throughput of the proposed scheme scales as $ \Theta ( { \log n } ) $ . Notably , this is proven to be the optimal throughput scaling even if centralized scheduling is allowed , thus proving the optimality of the proposed scheme in the scaling law sense .
0712.1182	cs.AI cs.LO	Cumulative and Averaging Fission of Beliefs	Belief fusion is the principle of combining separate beliefs or bodies of evidence originating from different sources . Depending on the situation to be modelled , different belief fusion methods can be applied . Cumulative and averaging belief fusion is defined for fusing opinions in subjective logic , and for fusing belief functions in general . The principle of fission is the opposite of fusion , namely to eliminate the contribution of a specific belief from an already fused belief , with the purpose of deriving the remaining belief . This paper describes fission of cumulative belief as well as fission of averaging belief in subjective logic . These operators can for example be applied to belief revision in Bayesian belief networks , where the belief contribution of a given evidence source can be determined as a function of a given fused belief and its other contributing beliefs .
0712.1189	cs.PL cs.SE	Implementation , Compilation , Optimization of Object-Oriented Languages , Programs and Systems - Report on the Workshop ICOOOLPS'2007 at ECOOP'07	ICOOOLPS'2007 was the second edition of the ECOOP-ICOOOLPS workshop . ICOOOLPS intends to bring researchers and practitioners both from academia and industry together , with a spirit of openness , to try and identify and begin to address the numerous and very varied issues of optimization . After a first successful edition , this second one put a stronger emphasis on exchanges and discussions amongst the participants , progressing on the bases set last year in Nantes . The workshop attendance was a success , since the 30-people limit we had set was reached about 2 weeks before the workshop itself . Some of the discussions ( e.g . annotations ) were so successful that they would required even more time than we were able to dedicate to them . That 's one area we plan to further improve for the next edition .
0712.1205	cs.PL cs.CR	Lambda-RBAC : Programming with Role-Based Access Control	We study mechanisms that permit program components to express role constraints on clients , focusing on programmatic security mechanisms , which permit access controls to be expressed , in situ , as part of the code realizing basic functionality . In this setting , two questions immediately arise : ( 1 ) The user of a component faces the issue of safety : is a particular role sufficient to use the component ? ( 2 ) The component designer faces the dual issue of protection : is a particular role demanded in all execution paths of the component ? We provide a formal calculus and static analysis to answer both questions .
0712.1224	cs.CR	Evaluating the Utility of Anonymized Network Traces for Intrusion Detection	Anonymization is the process of removing or hiding sensitive information in logs . Anonymization allows organizations to share network logs while not exposing sensitive information . However , there is an inherent trade off between the amount of information revealed in the log and the usefulness of the log to the client ( the utility of a log ) . There are many anonymization techniques , and there are many ways to anonymize a particular log ( that is , which fields to anonymize and how ) . Different anonymization policies will result in logs with varying levels of utility for analysis . In this paper we explore the effect of different anonymization policies on logs . We provide an empirical analysis of the effect of varying anonymization policies by looking at the number of alerts generated by an Intrusion Detection System . This is the first work to thoroughly evaluate the effect of single field anonymization policies on a data set . Our main contributions are to determine a set of fields that have a large impact on the utility of a log .
