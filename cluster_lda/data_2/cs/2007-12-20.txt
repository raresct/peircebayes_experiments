0712.3329	cs.AI	Universal Intelligence : A Definition of Machine Intelligence	A fundamental problem in artificial intelligence is that nobody really knows what intelligence is . The problem is especially acute when we need to consider artificial systems which are significantly different to humans . In this paper we approach this problem in the following way : We take a number of well known informal definitions of human intelligence that have been given by experts , and extract their essential features . These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines . We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense . We then show how this formal definition is related to the theory of universal optimal learning agents . Finally , we survey the many other tests and definitions of intelligence that have been proposed for machines .
0712.3331	cs.DM cs.CG	How to Complete a Doubling Metric	In recent years , considerable advances have been made in the study of properties of metric spaces in terms of their doubling dimension . This line of research has not only enhanced our understanding of finite metrics , but has also resulted in many algorithmic applications . However , we still do not understand the interaction between various graph-theoretic ( topological ) properties of graphs , and the doubling ( geometric ) properties of the shortest-path metrics induced by them . For instance , the following natural question suggests itself : \emph { given a finite doubling metric $ ( V , d ) $ , is there always an \underline { unweighted } graph $ ( V ' , E ' ) $ with $ V\subseteq V ' $ such that the shortest path metric $ d ' $ on $ V ' $ is still doubling , and which agrees with $ d $ on $ V $ . } This is often useful , given that unweighted graphs are often easier to reason about . We show that for any metric space $ ( V , d ) $ , there is an \emph { unweighted } graph $ ( V ' , E ' ) $ with shortest-path metric $ d ' : V'\times V ' \to \R_ { \geq 0 } $ such that -- for all $ x , y \in V $ , the distances $ d ( x , y ) \leq d ' ( x , y ) \leq ( 1+\eps ) \cdot d ( x , y ) $ , and -- the doubling dimension for $ d ' $ is not much more than that of $ d $ , where this change depends only on $ \e $ and not on the size of the graph . We show a similar result when both $ ( V , d ) $ and $ ( V ' , E ' ) $ are restricted to be trees : this gives a simpler proof that doubling trees embed into constant dimensional Euclidean space with constant distortion . We also show that our results are tight in terms of the tradeoff between distortion and dimension blowup .
0712.3333	cs.DS cs.DM	On the approximability of the vertex cover and related problems	In this paper we show that the problem of identifying an edge $ ( i , j ) $ in a graph $ G $ such that there exists an optimal vertex cover $ S $ of $ G $ containing exactly one of the nodes $ i $ and $ j $ is NP-hard . Such an edge is called a weak edge . We then develop a polynomial time approximation algorithm for the vertex cover problem with performance guarantee $ 2-\frac { 1 } { 1+\sigma } $ , where $ \sigma $ is an upper bound on a measure related to a weak edge of a graph . Further , we discuss a new relaxation of the vertex cover problem which is used in our approximation algorithm to obtain smaller values of $ \sigma $ . We also obtain linear programming representations of the vertex cover problem for special graphs . Our results provide new insights into the approximability of the vertex cover problem - a long standing open problem .
0712.3335	cs.DS cs.DM	A polynomial time $ \frac 3 2 $ -approximation algorithm for the vertex cover problem on a class of graphs	We develop a polynomial time 3/2-approximation algorithm to solve the vertex cover problem on a class of graphs satisfying a property called `` active edge hypothesis '' . The algorithm also guarantees an optimal solution on specially structured graphs . Further , we give an extended algorithm which guarantees a vertex cover $ S_1 $ on an arbitrary graph such that $ |S_1|\leq { 3/2 } |S^*|+\xi $ where $ S^* $ is an optimal vertex cover and $ \xi $ is an error bound identified by the algorithm . We obtained $ \xi = 0 $ for all the test problems we have considered which include specially constructed instances that were expected to be hard . So far we could not construct a graph that gives $ \xi \not= 0 $ .
0712.3348	cs.CC	On Exponential Time Lower Bound of Knapsack under Backtracking	M.Aleknovich et al . have recently proposed a model of algorithms , called BT model , which generalizes both the priority model of Borodin , Nielson and Rackoff , as well as a simple dynamic programming model by Woeginger . BT model can be further divided into three kinds of fixed , adaptive and fully adaptive ones . They have proved exponential time lower bounds of exact and approximation algorithms under adaptive BT model for Knapsack problem . Their exact lower bound is $ \Omega ( 2^ { 0.5n } /\sqrt { n } ) $ , in this paper , we slightly improve the exact lower bound to about $ \Omega ( 2^ { 0.69n } /\sqrt { n } ) $ , by the same technique , with related parameters optimized .
0712.3360	cs.DS	Compressed Text Indexes : From Theory to Practice !	A compressed full-text self-index represents a text in a compressed form and still answers queries efficiently . This technology represents a breakthrough over the text indexing techniques of the previous decade , whose indexes required several times the size of the text . Although it is relatively new , this technology has matured up to a point where theoretical research is giving way to practical developments . Nonetheless this requires significant programming skills , a deep engineering effort , and a strong algorithmic background to dig into the research results . To date only isolated implementations and focused comparisons of compressed indexes have been reported , and they missed a common API , which prevented their re-use or deployment within other applications . The goal of this paper is to fill this gap . First , we present the existing implementations of compressed indexes from a practitioner 's point of view . Second , we introduce the Pizza & Chili site , which offers tuned implementations and a standardized API for the most successful compressed full-text self-indexes , together with effective testbeds and scripts for their automatic validation and test . Third , we show the results of our extensive experiments on these codes with the aim of demonstrating the practical relevance of this novel and exciting technology .
0712.3380	cs.LO	Extending the Overlap Graph for Gene Assembly in Ciliates	Gene assembly is an intricate biological process that has been studied formally and modeled through string and graph rewriting systems . Recently , a restriction of the general ( intramolecular ) model , called simple gene assembly , has been introduced . This restriction has subsequently been defined as a string rewriting system . We show that by extending the notion of overlap graph it is possible to define a graph rewriting system for two of the three types of rules that make up simple gene assembly . It turns out that this graph rewriting system is less involved than its corresponding string rewriting system . Finally , we give characterizations of the `power ' of both types of graph rewriting rules . Because of the equivalence of these string and graph rewriting systems , the given characterizations can be carried over to the string rewriting system .
0712.3389	cs.DC cs.PF	RZBENCH : Performance evaluation of current HPC architectures using low-level and application benchmarks	RZBENCH is a benchmark suite that was specifically developed to reflect the requirements of scientific supercomputer users at the University of Erlangen-Nuremberg ( FAU ) . It comprises a number of application and low-level codes under a common build infrastructure that fosters maintainability and expandability . This paper reviews the structure of the suite and briefly introduces the most relevant benchmarks . In addition , some widely known standard benchmark codes are reviewed in order to emphasize the need for a critical review of often-cited performance results . Benchmark data is presented for the HLRB-II at LRZ Munich and a local InfiniBand Woodcrest cluster as well as two uncommon system architectures : A bandwidth-optimized InfiniBand cluster based on single socket nodes ( `` Port Townsend '' ) and an early version of Sun's highly threaded T2 architecture ( `` Niagara 2 '' ) .
0712.3402	cs.LG	Graph kernels between point clouds	Point clouds are sets of points in two or three dimensions . Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics . In this paper , we present extensions of graph kernels for point clouds , which allow to use kernel methods for such ob jects as shapes , line drawings , or any three-dimensional point clouds . In order to design rich and numerically efficient kernels with as few free parameters as possible , we use kernels between covariance matrices and their factorizations on graphical models . We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples .
0712.3423	cs.LO cs.CE	Tuplix Calculus	We introduce a calculus for tuplices , which are expressions that generalize matrices and vectors . Tuplices have an underlying data type for quantities that are taken from a zero-totalized field . We start with the core tuplix calculus CTC for entries and tests , which are combined using conjunctive composition . We define a standard model and prove that CTC is relatively complete with respect to it . The core calculus is extended with operators for choice , information hiding , scalar multiplication , clearing and encapsulation . We provide two examples of applications ; one on incremental financial budgeting , and one on modular financial budget design .
0712.3501	cs.IT math.IT	The Impact of Hard-Decision Detection on the Energy Efficiency of Phase and Frequency Modulation	The central design challenge in next generation wireless systems is to have these systems operate at high bandwidths and provide high data rates while being cognizant of the energy consumption levels especially in mobile applications . Since communicating at very high data rates prohibits obtaining high bit resolutions from the analog-to-digital ( A/D ) converters , analysis of the energy efficiency under the assumption of hard-decision detection is called for to accurately predict the performance levels . In this paper , transmission over the additive white Gaussian noise ( AWGN ) channel , and coherent and noncoherent fading channels is considered , and the impact of hard-decision detection on the energy efficiency of phase and frequency modulations is investigated . Energy efficiency is analyzed by studying the capacity of these modulation schemes and the energy required to send one bit of information reliably in the low signal-to-noise ratio ( SNR ) regime . The capacity of hard-decision-detected phase and frequency modulations is characterized at low SNR levels through closed-form expressions for the first and second derivatives of the capacity at zero SNR . Subsequently , bit energy requirements in the low-SNR regime are identified . The increases in the bit energy incurred by hard-decision detection and channel fading are quantified . Moreover , practical design guidelines for the selection of the constellation size are drawn from the analysis of the spectral efficiency -- bit energy tradeoff .
0712.3568	cs.DS	A Partition-Based Relaxation For Steiner Trees	The Steiner tree problem is a classical NP-hard optimization problem with a wide range of practical applications . In an instance of this problem , we are given an undirected graph G= ( V , E ) , a set of terminals R , and non-negative costs c_e for all edges e in E. Any tree that contains all terminals is called a Steiner tree ; the goal is to find a minimum-cost Steiner tree . The nodes V R are called Steiner nodes . The best approximation algorithm known for the Steiner tree problem is due to Robins and Zelikovsky ( SIAM J. Discrete Math , 2005 ) ; their greedy algorithm achieves a performance guarantee of 1+ ( ln 3 ) /2 ~ 1.55 . The best known linear ( LP ) -based algorithm , on the other hand , is due to Goemans and Bertsimas ( Math . Programming , 1993 ) and achieves an approximation ratio of 2-2/|R| . In this paper we establish a link between greedy and LP-based approaches by showing that Robins and Zelikovsky 's algorithm has a natural primal-dual interpretation with respect to a novel partition-based linear programming relaxation . We also exhibit surprising connections between the new formulation and existing LPs and we show that the new LP is stronger than the bidirected cut formulation . An instance is b-quasi-bipartite if each connected component of G R has at most b vertices . We show that Robins ' and Zelikovsky 's algorithm has an approximation ratio better than 1+ ( ln 3 ) /2 for such instances , and we prove that the integrality gap of our LP is between 8/7 and ( 2b+1 ) / ( b+1 ) .
0712.3587	cs.IT cs.CV math.IT	Pattern Recognition System Design with Linear Encoding for Discrete Patterns	In this paper , designs and analyses of compressive recognition systems are discussed , and also a method of establishing a dual connection between designs of good communication codes and designs of recognition systems is presented . Pattern recognition systems based on compressed patterns and compressed sensor measurements can be designed using low-density matrices . We examine truncation encoding where a subset of the patterns and measurements are stored perfectly while the rest is discarded . We also examine the use of LDPC parity check matrices for compressing measurements and patterns . We show how more general ensembles of good linear codes can be used as the basis for pattern recognition system design , yielding system design strategies for more general noise models .
0712.3617	cs.CE	A Unified Framework for Pricing Credit and Equity Derivatives	We propose a model which can be jointly calibrated to the corporate bond term structure and equity option volatility surface of the same company . Our purpose is to obtain explicit bond and equity option pricing formulas that can be calibrated to find a risk neutral model that matches a set of observed market prices . This risk neutral model can then be used to price more exotic , illiquid or over-the-counter derivatives . We observe that the model implied credit default swap ( CDS ) spread matches the market CDS spread and that our model produces a very desirable CDS spread term structure . This is observation is worth noticing since without calibrating any parameter to the CDS spread data , it is matched by the CDS spread that our model generates using the available information from the equity options and corporate bond markets . We also observe that our model matches the equity option implied volatility surface well since we properly account for the default risk premium in the implied volatility surface . We demonstrate the importance of accounting for the default risk and stochastic interest rate in equity option pricing by comparing our results to Fouque , Papanicolaou , Sircar and Solna ( 2003 ) , which only accounts for stochastic volatility .
