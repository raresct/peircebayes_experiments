0712.4332	q-bio.GN q-bio.PE	Comparing a Menagerie of Models for Estimating Molecular Divergence Times	Estimation of molecular evolutionary divergence times requires models of rate change . These vary with regard to the assumption of what quantity is penalized . The possibilities considered are the rate of evolution , the log of the rate of evolution and the inverse of the rate of evolution . These models also vary with regard to how time affects the expected variance of rate change . Here the alternatives are not at all , linearly with time and as the product of rate and time . This results in a set of nine models , both random walks and Brownian motion . A priori any of these models could be correct , yet different researchers may well prefer , or simply use , one rather than the others . Another variable is whether to use a scaling factor to take account of the variance of the process of rate change being unknown and therefore avoid minimizing the penalty function with unrealistically large times . Here the difference these models and assumptions make on a tree of mammals , with the root fixed and with a single internal node fixed , is measured . The similarity of models is measured as the correlation of their time estimates and visualized with a least squares tree . The fit of model to data is measured and Q-Q plots are shown . Comparing model estimates with each other , the age of clades within Laurasiatheria are seen to vary far more across models than those within Supraprimates ( informally called Euarchontoglires ) . Especially problematic are the often-used fossil calibrated nodes of horse/rhino and whale/hippo clashing with times within Supraprimates and in particular no fossil rodent teeth older than ~60 mybp . A scaling factor in addition to penalizing rate change is seen to yield consistent relative time estimates irrespective of exactly where the calibration point is placed .
0712.4381	q-bio.NC	Efficient representation as a design principle for neural coding and computation	Does the brain construct an efficient representation of the sensory world ? We review progress on this question , focusing on a series of experiments in the last decade which use fly vision as a model system in which theory and experiment can confront each other . Although the idea of efficient representation has been productive , clearly it is incomplete since it doesn't tell us which bits of sensory information are most valuable to the organism . We suggest that an organism which maximizes the ( biologically meaningful ) adaptive value of its actions given fixed resources should have internal representations of the outside world that are optimal in a very specific information theoretic sense : they maximize the information about the future of sensory inputs at a fixed value of the information about their past . This principle contains as special cases computations which the brain seems to carry out , and it should be possible to test this optimization directly . We return to the fly visual system and report the results of preliminary experiments that are in encouraging agreement with theory .
0712.4382	q-bio.PE	Information and fitness	The growth rate of organisms depends both on external conditions and on internal states , such as the expression levels of various genes . We show that to achieve a criterion mean growth rate over an ensemble of conditions , the internal variables must carry a minimum number of bits of information about those conditions . Evolutionary competition thus can select for cellular mechanisms that are more efficient in an abstract , information theoretic sense . Estimates based on recent experiments suggest that the minimum information required for reasonable growth rates is close to the maximum information that can be conveyed through biologically realistic regulatory mechanisms . These ideas are applicable most directly to unicellular organisms , but there are analogies to problems in higher organisms , and we suggest new experiments for both cases .
0712.4385	q-bio.MN	Cell biology : Networks , regulation , pathways	This review was written for the Encyclopedia of Complexity and System Science ( Springer-Verlag , Berlin , 2008 ) , and is intended as a guide to the growing literature which approaches the phenomena of cell biology from a more theoretical point of view . We begin with the building blocks of cellular networks , and proceed toward the different classes of models being explored , finally discussing the `` design principles '' which have been suggested for these systems . Although largely a dispassionate review , we do draw attention to areas where there seems to be general consensus on ideas that have not been tested very thoroughly and , more optimistically , to areas where we feel promising ideas deserve to be more fully explored .
0712.4397	q-bio.QM	Rediscovering the power of pairwise interactions	Two recent streams of work suggest that pairwise interactions may be sufficient to capture the complexity of biological systems ranging from protein structure to networks of neurons . In one approach , possible amino acid sequences in a family of proteins are generated by Monte Carlo annealing of a '' Hamiltonian '' that forces pairwise correlations among amino acid substitutions to be close to the observed correlations . In the other approach , the observed correlations among pairs of neurons are used to construct a maximum entropy model for the states of the network as a whole . We show that , in certain limits , these two approaches are mathematically equivalent , and we comment on open problems suggested by this framework
0801.0011	q-bio.NC	How synchronization protects from noise	Synchronization phenomena are pervasive in biology . In neuronal networks , the mechanisms of synchronization have been extensively studied from both physiological and computational viewpoints . The functional role of synchronization has also attracted much interest and debate . In particular , synchronization may allow distant sites in the brain to communicate and cooperate with each other , and therefore it may play a role in temporal binding and in attention and sensory-motor integration mechanisms . In this article , we study another role for synchronization : the so-called '' collective enhancement of precision . '' We argue , in a full nonlinear dynamical context , that synchronization may help protect interconnected neurons from the influence of random perturbations -- intrinsic neuronal noise -- which affect all neurons in the nervous system . This property may allow reliable computations to be carried out even in the presence of significant noise ( as experimentally found e.g. , in retinal ganglion cells in primates ) , as mathematically it is key to obtaining meaningful downstream signals , whether in terms of precisely-timed interaction ( temporal coding ) , population coding , or frequency coding . Using stochastic contraction theory , we show how synchronization of nonlinear dynamical systems helps protect these systems from random perturbations . Our main contribution is a mathematical proof that , under specific quantified conditions , the impact of noise on each individual system and on the spatial mean can essentially be cancelled through synchronization . Similar concepts may be applicable to questions in systems biology .
